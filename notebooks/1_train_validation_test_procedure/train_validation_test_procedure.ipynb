{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bd9f332",
   "metadata": {},
   "source": [
    "# 1. Train validation test procedure\n",
    "\n",
    "The provided code defines a `Model` class which is used to create, train, validate, and save a model aiming to identify pneumonia chest x-ray.\n",
    "\n",
    "All differents elaborate models will follow this organization:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef221de1",
   "metadata": {},
   "source": [
    "\n",
    "The `__init__` method initializes the class. \n",
    "It sets up paths to the training data and creates `Dataset` objects for both the training and validation data. \n",
    "\n",
    "These datasets are then built using TensorFlow's `AUTOTUNE` functionality for optimized data loading.\n",
    "\n",
    "Various information about the data, such as the class names and batch shapes, is then printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bcdb3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from x_ray_dataset_builder import Dataset\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        train_dir = pathlib.Path(\"data/train\")\n",
    "\n",
    "        train_ds = Dataset(train_dir, 0.2, \"training\")\n",
    "        val_ds = Dataset(train_dir, 0.2, \"validation\")\n",
    "\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "        train_ds.build(AUTOTUNE, True)\n",
    "        val_ds.build(AUTOTUNE)\n",
    "\n",
    "        class_names = train_ds.get_class_names()\n",
    "        print(\"\\nClass names:\")\n",
    "        print(class_names)\n",
    "\n",
    "        train_x_batch_shape = train_ds.get_x_batch_shape()\n",
    "        print(\"\\nTraining dataset's images batch shape is:\")\n",
    "        print(train_x_batch_shape)\n",
    "\n",
    "        train_y_batch_shape = train_ds.get_y_batch_shape()\n",
    "        print(\"\\nTraining dataset's labels batch shape is:\")\n",
    "        print(train_y_batch_shape)\n",
    "\n",
    "        train_ds.display_images_in_batch(2, \"Training dataset\")\n",
    "        train_ds.display_batch_number(\"Training dataset\")\n",
    "        train_ds.display_distribution(\"Training dataset\")\n",
    "        train_ds.display_mean(\"Training dataset\")\n",
    "\n",
    "        val_x_batch_shape = train_ds.get_x_batch_shape()\n",
    "        print(\"\\nTesting dataset's images batch shape is:\")\n",
    "        print(val_x_batch_shape)\n",
    "\n",
    "        val_y_batch_shape = train_ds.get_y_batch_shape()\n",
    "        print(\"\\nTesting dataset's labels batch shape is:\")\n",
    "        print(val_y_batch_shape)\n",
    "\n",
    "        val_ds.display_images_in_batch(2, \"Testing dataset\")\n",
    "        val_ds.display_batch_number(\"Testing dataset\")\n",
    "        val_ds.display_distribution(\"Testing dataset\")\n",
    "        val_ds.display_mean(\"Testing dataset\")\n",
    "\n",
    "        self.class_names = class_names\n",
    "        self.train_ds = train_ds.normalized_dataset\n",
    "        self.val_ds = val_ds.normalized_dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4a67aa9",
   "metadata": {},
   "source": [
    "\n",
    "The `build` method defines and compiles the model.\n",
    "\n",
    "The model is a simple feed-forward neural network (also known as a multi-layer perceptron or MLP) with one hidden layer of 128 neurons. The goal is not to build a model for performances but to implement a first train, validation, test procedure.\n",
    "\n",
    "The input data are flattened before being passed through the network.\n",
    "\n",
    "The output layer uses a softmax activation function, which is standard for multi-class classification problems.\n",
    "\n",
    "The model is compiled with the Adam optimizer and categorical cross-entropy loss, which is also standard for such tasks. \n",
    "\n",
    "Four metrics are monitored during training: categorical accuracy, precision, recall, and AUC (Area Under the ROC Curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34449231",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    def build(self):\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(input_shape=(180, 180, 1)),\n",
    "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(len(self.class_names), activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "            metrics=[\n",
    "                tf.keras.metrics.CategoricalAccuracy(), \n",
    "                tf.keras.metrics.Precision(), \n",
    "                tf.keras.metrics.Recall(),\n",
    "                tf.keras.metrics.AUC()\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f10b5de",
   "metadata": {},
   "source": [
    "\n",
    "The `train` method trains the model for a specified number of epochs.\n",
    "\n",
    "The model's accuracy and loss on both the training and validation data are plotted after each epoch.\n",
    "\n",
    "This allows for the monitoring of the model's performance over time and the detection of any overfitting (where the model performs well on the training data but poorly on the validation data).\n",
    "\n",
    "After training, the model is saved in both Keras' native format and in a format compatible with TensorFlow.js, which enables the model to be used in a web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9997bd29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    def train(self, epochs):\n",
    "        model = self.build()\n",
    "\n",
    "        print(\"\\nStarting training...\")\n",
    "        history = model.fit(self.train_ds, validation_data=self.val_ds, epochs=epochs)\n",
    "        print(\"\\n\\033[92mTraining done !\\033[0m\")\n",
    "\n",
    "        acc = history.history[\"categorical_accuracy\"]\n",
    "        val_acc = history.history[\"val_categorical_accuracy\"]\n",
    "\n",
    "        loss = history.history[\"loss\"]\n",
    "        val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "        epochs_range = range(epochs)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs_range, acc, label=\"Training Categorical Accuracy\")\n",
    "        plt.plot(epochs_range, val_acc, label=\"Validation Categorical Accuracy\")\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.title(\"Training and Validation Categorical Accuracy\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs_range, loss, label=\"Training Loss\")\n",
    "        plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nSaving...\")\n",
    "        model.save(\"notebooks/1_train_validation_test_procedure/model_1.keras\")\n",
    "        tfjs.converters.save_keras_model(model, \"notebooks/1_train_validation_test_procedure\")\n",
    "        print(\"\\n\\033[92mSaving done !\\033[0m\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "952fd022",
   "metadata": {},
   "source": [
    "\n",
    "In summary, this code provides a full procedure for training a machine learning model, including data loading, model creation, training, validation, and saving. \n",
    "\n",
    "The procedure is specific to a binary classification task, but could be adapted for other types of tasks. \n",
    "\n",
    "The use of a validation dataset allows for the monitoring of the model's performance on unseen data during training, which can help prevent overfitting.\n",
    "This dataset is build as a subset of the data/train directory. Idem for the training dataset. For the test datasets, it's accessed only during evaluation phase.\n",
    "This allow us to detect overfiting or bad performances and correspond to the principe of the train, validation, test procedure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
