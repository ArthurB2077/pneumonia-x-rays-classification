{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from scripts.x_ray_dataset_builder import Dataset\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(256, 256, 1)))\n",
    "\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "    hp_activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(units=hp_units, activation=hp_activation))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    hp_learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "\n",
    "    optimizer_func = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "\n",
    "    loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer_func,\n",
    "        loss=loss_func, \n",
    "        metrics=[\n",
    "            tf.keras.metrics.CategoricalAccuracy(), \n",
    "            tf.keras.metrics.Precision(), \n",
    "            tf.keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "class HyperModel:\n",
    "    def __init__(self, x_train, y_train, max_epochs):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.max_epochs = max_epochs\n",
    "        self.hypermodel = None\n",
    "\n",
    "    def build(self):\n",
    "        stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "        hyper_band = kt.Hyperband(model_builder, objective=kt.Objective('val_recall', direction='max'), max_epochs=50, factor=3, directory='hypertunning_logs', project_name='hyperband_algo')\n",
    "        class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(self.y_train), y=np.argmax(self.y_train, axis=1))\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "        class_weights[0] = class_weights[0] * 4\n",
    "        hyper_band.search(self.x_train, self.y_train, class_weight=class_weights, validation_split=0.20, callbacks=[stop_early], epochs=self.max_epochs, batch_size=32)\n",
    "        print(\"\\n\")\n",
    "        hyper_band.results_summary(1)\n",
    "        best_hyperparameters = hyper_band.get_best_hyperparameters()[0]\n",
    "        hypermodel = hyper_band.hypermodel.build(best_hyperparameters)\n",
    "        self.hypermodel = hypermodel\n",
    "\n",
    "        return hypermodel\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, image_size=(512, 512)):\n",
    "        train_dir = pathlib.Path(\"data/train\")\n",
    "\n",
    "        train_ds = Dataset(train_dir, batch_size=32, image_size=image_size)\n",
    "\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "        train_ds.build(AUTOTUNE, False)\n",
    "\n",
    "        class_names = train_ds.get_class_names()\n",
    "        print(\"\\nClass names:\")\n",
    "        print(class_names)\n",
    "\n",
    "        train_x_batch_shape = train_ds.get_x_batch_shape()\n",
    "        print(\"\\nTraining dataset's images batch shape is:\")\n",
    "        print(train_x_batch_shape)\n",
    "\n",
    "        train_y_batch_shape = train_ds.get_y_batch_shape()\n",
    "        print(\"\\nTraining dataset's labels batch shape is:\")\n",
    "        print(train_y_batch_shape)\n",
    "\n",
    "        train_ds.display_images_in_batch(1, \"Training dataset\")\n",
    "        train_ds.display_batch_number(\"Training dataset\")\n",
    "        train_ds.display_distribution(\"Training dataset\")\n",
    "        train_ds.display_mean(\"Training dataset\")\n",
    "\n",
    "        self.class_names = class_names\n",
    "        self.train_ds = train_ds.normalized_dataset\n",
    "        self.x_train = train_ds.x_dataset\n",
    "        self.y_train = train_ds.y_dataset\n",
    "\n",
    "\n",
    "    def train(self, epochs, max_epochs, k=5):\n",
    "        hypermodel = HyperModel(self.x_train, self.y_train, max_epochs)\n",
    "\n",
    "        k = k\n",
    "        kfold = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "        fold = 1\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(self.y_train), y=np.argmax(self.y_train, axis=1))\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "        class_weights[0] = class_weights[0] * 6.75\n",
    "\n",
    "\n",
    "        for train_index, val_index in kfold.split(self.x_train, self.y_train):       \n",
    "            model = hypermodel.build()\n",
    "\n",
    "            print(f\"\\nProcessing fold {fold}\")\n",
    "            train_images, val_images = self.x_train[train_index], self.x_train[val_index]\n",
    "            train_labels, val_labels = self.y_train[train_index], self.y_train[val_index]\n",
    "\n",
    "            history = model.fit(train_images, train_labels, class_weight=class_weights, batch_size=32, epochs=epochs, validation_data=(val_images, val_labels))\n",
    "            \n",
    "            fold += 1\n",
    "\n",
    "            categorical_accuracy = history.history[\"categorical_accuracy\"]\n",
    "            val_categorical_accuracy = history.history[\"val_categorical_accuracy\"]\n",
    "\n",
    "            loss = history.history[\"loss\"]\n",
    "            val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "            epochs_range = range(epochs)\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(epochs_range, categorical_accuracy, label=\"Training Accuracy\")\n",
    "            plt.plot(epochs_range, val_categorical_accuracy, label=\"Validation Accuracy\")\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epochs_range, loss, label=\"Training Loss\")\n",
    "            plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.title(\"Training and Validation Loss\")\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"\\n\\033[92mTraining done !\\033[0m\")\n",
    "\n",
    "        print(\"\\nSaving...\")\n",
    "        model.save(\"notebooks/3_tuning_parameters_on_training_set/model_3.keras\")\n",
    "        tfjs.converters.save_keras_model(model, \"notebooks/3_tuning_parameters_on_training_set\")\n",
    "        print(\"\\n\\033[92mSaving done !\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
