{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01ab9768",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "\n",
    "This a first demonstration of how we'll use cross validation method in other models. Performance is not yet the main concern. This aim to clearely defined what is cross validation and how to apply it in any type of model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c8f9f91",
   "metadata": {},
   "source": [
    "The Model class is defined with methods for initializing the dataset, building the model, and training the model.\n",
    "\n",
    "The __init__ method initializes the dataset from the \"data/train\" directory, gets the class names and some dataset properties, and displays some information about the dataset, such as image batch shapes, class distribution, and the mean of the data. \n",
    "\n",
    "The dataset is normalized and the features (images) and labels (classes) are separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a766f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scripts.x_ray_dataset_builder import Dataset\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, image_size=(512, 512)):\n",
    "        train_dir = pathlib.Path(\"data/train\")\n",
    "\n",
    "        train_ds = Dataset(train_dir, batch_size=64, image_size=image_size)\n",
    "\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "        train_ds.build(AUTOTUNE, False)\n",
    "\n",
    "        class_names = train_ds.get_class_names()\n",
    "        print(\"\\nClass names:\")\n",
    "        print(class_names)\n",
    "\n",
    "        train_x_batch_shape = train_ds.get_x_batch_shape()\n",
    "        print(\"\\nTraining dataset's images batch shape is:\")\n",
    "        print(train_x_batch_shape)\n",
    "\n",
    "        train_y_batch_shape = train_ds.get_y_batch_shape()\n",
    "        print(\"\\nTraining dataset's labels batch shape is:\")\n",
    "        print(train_y_batch_shape)\n",
    "\n",
    "        train_ds.display_images_in_batch(2, \"Training dataset\")\n",
    "        train_ds.display_batch_number(\"Training dataset\")\n",
    "        train_ds.display_distribution(\"Training dataset\")\n",
    "        train_ds.display_mean(\"Training dataset\")\n",
    "\n",
    "        self.class_names = class_names\n",
    "        self.train_ds = train_ds.normalized_dataset\n",
    "        self.x_train = train_ds.x_dataset\n",
    "        self.y_train = train_ds.y_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac5fc10e",
   "metadata": {},
   "source": [
    "The build method defines the architecture of the model, which is a simple feedforward neural network with an input layer that flattens the images, a hidden layer with 128 neurons using ReLU activation function, and an output layer with a number of neurons equivalent to the number of classes using the softmax activation function. \n",
    "\n",
    "The model is compiled with the Adam optimizer, categorical cross-entropy loss function, and metrics including categorical accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    " def build(self, input_shape=(512, 512, 1)):\n",
    "        model = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "                tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "                tf.keras.layers.Dense(len(self.class_names), activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "            metrics=[\n",
    "                tf.keras.metrics.CategoricalAccuracy(), \n",
    "                tf.keras.metrics.Precision(), \n",
    "                tf.keras.metrics.Recall()\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4ef8e46",
   "metadata": {},
   "source": [
    "The train method implements k-fold cross-validation for training and evaluating the model. \n",
    "\n",
    "The dataset is split into 'k' folds, with each fold used as a validation set while the remaining folds are used for training. \n",
    "\n",
    "The script computes class weights for handling imbalanced classes. \n",
    "\n",
    "For each fold, the model is built and trained, and the training and validation accuracies and losses are plotted. \n",
    "\n",
    "After training, the model is saved in the \"notebooks/2_cross_validation\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917e6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, epochs, k=5, input_shape=(512, 512, 1)):\n",
    "        print(\"\\nStarting training...\")\n",
    "        k = k\n",
    "        num_epochs = epochs\n",
    "\n",
    "        kfold = KFold(n_splits=k, shuffle=True, random_state=1)\n",
    "        fold = 1\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(self.y_train), y=np.argmax(self.y_train, axis=1))\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "        class_weights[0] = class_weights[0] * 4.25\n",
    "\n",
    "\n",
    "        for train_index, val_index in kfold.split(self.x_train, self.y_train):       \n",
    "            model = self.build(input_shape=input_shape)\n",
    "\n",
    "            print(f\"Processing fold {fold}\")\n",
    "            train_images, val_images = self.x_train[train_index], self.x_train[val_index]\n",
    "            train_labels, val_labels = self.y_train[train_index], self.y_train[val_index]\n",
    "\n",
    "            history = model.fit(train_images, train_labels, class_weight=class_weights, batch_size=64, epochs=num_epochs, validation_data=(val_images, val_labels))\n",
    "            \n",
    "            fold += 1\n",
    "\n",
    "            categorical_accuracy = history.history[\"categorical_accuracy\"]\n",
    "            val_categorical_accuracy = history.history[\"val_categorical_accuracy\"]\n",
    "\n",
    "            loss = history.history[\"loss\"]\n",
    "            val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "            epochs_range = range(epochs)\n",
    "\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(epochs_range, categorical_accuracy, label=\"Training Accuracy\")\n",
    "            plt.plot(epochs_range, val_categorical_accuracy, label=\"Validation Accuracy\")\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(epochs_range, loss, label=\"Training Loss\")\n",
    "            plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.title(\"Training and Validation Loss\")\n",
    "            plt.show()\n",
    "        \n",
    "        print(\"\\n\\033[92mTraining done !\\033[0m\")\n",
    "\n",
    "        print(\"\\nSaving...\")\n",
    "        model.save(\"notebooks/2_cross_validation/model_2.keras\")\n",
    "        model.save(\"notebooks/2_cross_validation\")\n",
    "        print(\"\\n\\033[92mSaving done !\\033[0m\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38045289",
   "metadata": {},
   "source": [
    "This script is an example of how to implement k-fold cross-validation in machine learning, which helps to better estimate the model's performance by averaging the performance metrics over 'k' different train-validation splits. \n",
    "\n",
    "It's particularly useful when the available data is limited"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
