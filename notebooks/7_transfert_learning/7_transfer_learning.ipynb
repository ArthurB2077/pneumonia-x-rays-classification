{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bd9f332",
   "metadata": {},
   "source": [
    "# Transfert learning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### What is transfert learning ?\n",
    "\n",
    "The idea is to reuse a pre-trained model to fill the lack of data. By using transfert learning we could benefit from all that model have learn on previous much larger dataset.\n",
    "\n",
    ">\"Transfer learning and domain adaptation refer to the situation where what has been learned in one setting… is exploited to improve generalization in another setting\" — Page 526, Deep Learning, 2016.\n",
    "\n",
    "In our specific case, we need model that are able to classify images. After have import the model we can refine it to become more specialised on our data without loosing the benefits of his already acquired experience.\n",
    "\n",
    "### Models we'll use\n",
    "\n",
    "As image classifier, we have selected three models to apply and experiment with:\n",
    "- [VGG16](https://www.mathworks.com/help/deeplearning/ref/vgg16.html)\n",
    "- [Resnet50](https://www.mathworks.com/help/deeplearning/ref/resnet50.html)\n",
    "- [InceptionV3](https://paperswithcode.com/method/inception-v3)\n",
    "\n",
    "Because it will be not fun to use only one model, we'll use the three models stacked.\n",
    "\n",
    "Now let's code !\n",
    "\n",
    "## The code\n",
    "\n",
    "### Transfert learning model\n",
    "\n",
    "First, we'll need the classical import that we used previously for our others models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52075de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from utils.x_ray_data_viz import plot_history\n",
    "from utils.x_ray_dataset_builder import Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3a69f17b",
   "metadata": {},
   "source": [
    "We also add `env variables` that allow us to configure our model's paramters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eedbf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = os.getenv(\"MODEL_ID\")\n",
    "BATCH_SIZE = int(os.getenv(f\"MODEL_{MODEL_ID}_BATCH_SIZE\"))\n",
    "CHART_DIR = pathlib.Path(os.getenv(f\"MODEL_{MODEL_ID}_CHART_DIR\")).absolute()\n",
    "MODEL_DIR = pathlib.Path(os.getenv(f\"MODEL_{MODEL_ID}_MODEL_DIR\")).absolute()\n",
    "IMG_COLOR = os.getenv(f\"MODEL_{MODEL_ID}_IMG_COLOR\")\n",
    "IMG_SIZE = int(os.getenv(f\"MODEL_{MODEL_ID}_IMG_SIZE\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79e67c89",
   "metadata": {},
   "source": [
    "In order to stack the three models we need to define a custom `MergeLayer` that herit from `tf.keras.layers/layer` and allow us to concatenate multiple output from the pretrained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b812ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MergeLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.keras.layers.Concatenate()(inputs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18516460",
   "metadata": {},
   "source": [
    "Now we can add our `Model` class and it's `__init__` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0005d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        img_size: int,\n",
    "        img_color: str,\n",
    "        label_mode: str,\n",
    "        interactive_reports: bool = True,\n",
    "    ):\n",
    "        train_path = pathlib.Path(\"data/train\")\n",
    "\n",
    "        test_dir = pathlib.Path(\"data/test\")\n",
    "\n",
    "        train_ds = Dataset(\n",
    "            train_path,\n",
    "            batch_size=batch_size,\n",
    "            color_mode=img_color,\n",
    "            image_size=img_size,\n",
    "            label_mode=label_mode,\n",
    "        )\n",
    "\n",
    "        test_ds = Dataset(\n",
    "            test_dir,\n",
    "            image_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            color_mode=img_color,\n",
    "            label_mode=label_mode,\n",
    "        )\n",
    "\n",
    "        AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "        train_ds.build(AUTOTUNE, True)\n",
    "\n",
    "        test_ds.build(AUTOTUNE)\n",
    "\n",
    "        class_weights = class_weight.compute_class_weight(\n",
    "            \"balanced\",\n",
    "            classes=np.unique(train_ds.y_dataset),\n",
    "            y=(train_ds.y_dataset > 0.5).astype(\"int32\").reshape(-1),\n",
    "        )\n",
    "\n",
    "        class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "        callback_stop_early = tf.keras.callbacks.EarlyStopping(\n",
    "            min_delta=0.01,\n",
    "            mode=\"max\",\n",
    "            monitor=\"val_binary_accuracy\",\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        callback_reduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            cooldown=2,\n",
    "            factor=0.1,\n",
    "            min_delta=0.01,\n",
    "            mode=\"max\",\n",
    "            monitor=\"binary_accuracy\",\n",
    "            patience=4,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        callback_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            MODEL_DIR.joinpath(\n",
    "                f\"checkpoints/model_7_checkpoint_{datetime.utcnow().isoformat()}.keras\"\n",
    "            ),\n",
    "            mode=\"max\",\n",
    "            monitor=\"val_binary_accuracy\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.callback_stop_early = callback_stop_early\n",
    "        self.callback_reduce_learning_rate = callback_reduce_learning_rate\n",
    "        self.callback_checkpoint = callback_checkpoint\n",
    "        self.class_names = train_ds.class_names\n",
    "        self.class_weights = class_weights\n",
    "        self.fold_acc = []\n",
    "        self.fold_loss = []\n",
    "        self.img_size = img_size\n",
    "        self.img_color = img_color\n",
    "        self.interactive_reports = interactive_reports\n",
    "        self.model = None\n",
    "        self.scores = []\n",
    "        self.x_train = train_ds.x_dataset\n",
    "        self.x_test = test_ds.x_dataset\n",
    "        self.y_train = train_ds.y_dataset\n",
    "        self.y_test = test_ds.y_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a708b02",
   "metadata": {},
   "source": [
    "We first load our train and test datasets. We define the weights applied to model to adjust imbalency. Finally we add our 3 training callbacks and save our important variable as class properties.\n",
    "\n",
    "Now lets define the build method by first load the 3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51946828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self):\n",
    "        input_shape = (self.img_size, self.img_size, 3)\n",
    "\n",
    "        input_layer = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "        vgg16 = tf.keras.applications.vgg16.VGG16(\n",
    "            include_top=False,\n",
    "            input_shape=input_shape,\n",
    "            weights=\"imagenet\",\n",
    "        )\n",
    "\n",
    "        inception_v3 = tf.keras.applications.inception_v3.InceptionV3(\n",
    "            include_top=False,\n",
    "            input_shape=input_shape,\n",
    "            weights=\"imagenet\",\n",
    "        )\n",
    "\n",
    "        resnet50 = tf.keras.applications.resnet50.ResNet50(\n",
    "            include_top=False,\n",
    "            input_shape=input_shape,\n",
    "            weights=\"imagenet\",\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71479071",
   "metadata": {},
   "source": [
    "We need to focus our attention on define `include_top` as `False` as we want to include the models in our own layer.\n",
    "\n",
    "After that, we need to freeze the layer so we can use it as a feature extractor.\n",
    "It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d940f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "        for layer in vgg16.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in inception_v3.layers:\n",
    "            layer.trainable = False\n",
    "        for layer in resnet50.layers:\n",
    "            layer.trainable = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a52fda8",
   "metadata": {},
   "source": [
    "Now nothing new, we'll use previous data augmentation technics to improve our model performances and prevent overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b50f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 1 if self.img_color == \"grayscale\" else 3\n",
    "\n",
    "        augmentation_layer = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.RandomRotation(\n",
    "                    0.3, input_shape=(IMG_SIZE, IMG_SIZE, channels)\n",
    "                ),\n",
    "                tf.keras.layers.RandomZoom(0.2),\n",
    "                tf.keras.layers.RandomTranslation((-0.1, 0.1), (-0.1, 0.1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        augmented_input_layer = augmentation_layer(input_layer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b55cfc18",
   "metadata": {},
   "source": [
    "It's time to add the models to our own model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e30342",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_outputs = vgg16(augmented_input_layer)\n",
    "vgg16_outputs = tf.keras.layers.GlobalAveragePooling2D()(vgg16_outputs)\n",
    "\n",
    "inception_v3_outputs = inception_v3(augmented_input_layer)\n",
    "inception_v3_outputs = tf.keras.layers.GlobalAveragePooling2D()(\n",
    "    inception_v3_outputs\n",
    ")\n",
    "\n",
    "resnet50_outputs = resnet50(augmented_input_layer)\n",
    "resnet50_outputs = tf.keras.layers.GlobalAveragePooling2D()(resnet50_outputs)\n",
    "\n",
    "trained_models_layer = MergeLayer()(\n",
    "    [vgg16_outputs, inception_v3_outputs, resnet50_outputs]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e58e1831",
   "metadata": {},
   "source": [
    "We just need to add a GlobalAveragePooling2D layer to each pretrained model output so they'll have the same output shape.\n",
    "\n",
    "Many models contain tf.keras.layers.BatchNormalization layers. This layer is a special case and precautions should be taken in the context of fine-tuning, as shown later in this tutorial.\n",
    "\n",
    "When you set layer.trainable = False, the BatchNormalization layer will run in inference mode, and will not update its mean and variance statistics.\n",
    "\n",
    "When you unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, you should keep the BatchNormalization layers in inference mode by passing training = False when calling the base model. Otherwise, the updates applied to the non-trainable weights will destroy what the model has learned.\n",
    "\n",
    "Now, we can finish our model structure by adding three more Dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    " model_outputs = tf.keras.layers.BatchNormalization()(trained_models_layer)\n",
    "        model_outputs = tf.keras.layers.Dense(\n",
    "            1024, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.0001)\n",
    "        )(model_outputs)\n",
    "        model_outputs = tf.keras.layers.Dropout(0.5)(model_outputs)\n",
    "\n",
    "        model_outputs = tf.keras.layers.BatchNormalization()(model_outputs)\n",
    "        model_outputs = tf.keras.layers.Dense(\n",
    "            512, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.0001)\n",
    "        )(model_outputs)\n",
    "        model_outputs = tf.keras.layers.Dropout(0.7)(model_outputs)\n",
    "\n",
    "        model_outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(model_outputs)\n",
    "\n",
    "        model = tf.keras.Model(inputs=input_layer, outputs=model_outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c25050d",
   "metadata": {},
   "source": [
    "Compile the model a first time but with his layers freeze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe45f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.BinaryAccuracy(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "    ],\n",
    "    optimizer=tf.keras.optimizers.AdamW(\n",
    "        amsgrad=True,\n",
    "        epsilon=0.0001,\n",
    "        learning_rate=0.004,\n",
    "    ),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    self.x_train,\n",
    "    self.y_train,\n",
    "    batch_size=self.batch_size,\n",
    "    callbacks=[self.callback_reduce_learning_rate, self.callback_stop_early],\n",
    "    class_weight=self.class_weights,\n",
    "    epochs=20,\n",
    "    shuffle=True,\n",
    "    validation_split=0.3,\n",
    ")\n",
    "\n",
    "model.evaluate(\n",
    "    self.x_test,\n",
    "    self.y_test,\n",
    "    batch_size=self.batch_size,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "self.model = model\n",
    "\n",
    "return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "969d47e2",
   "metadata": {},
   "source": [
    "Now each time we'll need to build the model, we'll need first to compile the pretrained model with his layers frozen.\n",
    "\n",
    "Implementing the train method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, epochs, k):\n",
    "        kfold = KFold(n_splits=k, shuffle=True)\n",
    "        fold_number = 1\n",
    "\n",
    "        for train, test in kfold.split(self.x_train, self.y_train):\n",
    "            print(\n",
    "                \"\\033[91m\"\n",
    "                \"=================================================================\\n\"\n",
    "                \"****STARTING RESNET_50, VGG_16 and INCEPTION_V3 PRE-TRAINING*****\\n\"\n",
    "                \"=================================================================\"\n",
    "                \"\\033[0m\"\n",
    "            )\n",
    "\n",
    "            train_images, val_images = self.x_train[train], self.x_train[test]\n",
    "            train_labels, val_labels = self.y_train[train], self.y_train[test]\n",
    "\n",
    "            print(\"\\033[96mBuilding model...\\n\")\n",
    "\n",
    "            pretrained_model = self.build()\n",
    "\n",
    "            print(\"\\033[0m\")\n",
    "\n",
    "            print(\n",
    "                \"\\n\\033[91m\"\n",
    "                \"=================================================================\\n\"\n",
    "                \"****PRE-TRAINING DONE FOR RESNET_50, VGG_16 and INCEPTION_V3*****\\n\"\n",
    "                \"=================================================================\"\n",
    "                \"\\033[0m\\n\"\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \"\\033[91m\"\n",
    "                \"=================================================================\\n\"\n",
    "                f\"**************STARTING TRAINING K-FOLD N°{fold_number}***********\\n\"\n",
    "                \"=================================================================\"\n",
    "                \"\\033[0m\"\n",
    "            )\n",
    "\n",
    "            for layer in pretrained_model.layers:\n",
    "                layer.trainable = True\n",
    "\n",
    "            print(\"\\n\\033[96mModel summary:\\033[0m\\n\")\n",
    "\n",
    "            pretrained_model.summary()\n",
    "\n",
    "            model = pretrained_model\n",
    "\n",
    "            print(\"\\n\\033[96m\")\n",
    "\n",
    "            model.compile(\n",
    "                loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2),\n",
    "                metrics=[\n",
    "                    tf.keras.metrics.BinaryAccuracy(),\n",
    "                    tf.keras.metrics.Precision(),\n",
    "                    tf.keras.metrics.Recall(),\n",
    "                ],\n",
    "                optimizer=tf.keras.optimizers.AdamW(\n",
    "                    amsgrad=True,\n",
    "                    epsilon=0.01,\n",
    "                    learning_rate=0.0004,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            history = model.fit(\n",
    "                train_images,\n",
    "                train_labels,\n",
    "                batch_size=self.batch_size,\n",
    "                callbacks=[\n",
    "                    self.callback_stop_early,\n",
    "                    self.callback_reduce_learning_rate,\n",
    "                    self.callback_checkpoint,\n",
    "                ],\n",
    "                class_weight=self.class_weights,\n",
    "                epochs=epochs,\n",
    "                shuffle=True,\n",
    "                validation_data=(val_images, val_labels),\n",
    "            )\n",
    "\n",
    "            self.scores = model.evaluate(\n",
    "                self.x_test, self.y_test, batch_size=self.batch_size, verbose=1\n",
    "            )\n",
    "\n",
    "            self.fold_acc.append(self.scores[1] * 100)\n",
    "            self.fold_loss.append(self.scores[0])\n",
    "\n",
    "            print(\"\\033[0m\")\n",
    "\n",
    "            print(\n",
    "                \"\\n\\033[91m\"\n",
    "                \"=================================================================\\n\"\n",
    "                f\"**************TRAINING FOR K-FOLD N°{fold_number} DONE!**********\\n\"\n",
    "                \"=================================================================\"\n",
    "                \"\\n\\033[0m\"\n",
    "            )\n",
    "\n",
    "            print(f\"\\n\\033[91mSaving model n°{fold_number}...\\033[0m\")\n",
    "            \n",
    "            self.model.save(MODEL_DIR.joinpath(f\"model_7_fold_{fold_number}.keras\"))\n",
    "\n",
    "            print(\"\\n\\033[92mSaving done !\\033[0m\")\n",
    "\n",
    "            plot_history(\n",
    "                history,\n",
    "                CHART_DIR.joinpath(\n",
    "                    f\"training_metrics/training_loss_and_accuracy_fold_{fold_number}.png\"\n",
    "                ),\n",
    "                accuracy_metric=\"binary_accuracy\",\n",
    "                interactive=self.interactive_reports,\n",
    "            )\n",
    "\n",
    "            fold_number = fold_number + 1\n",
    "\n",
    "        print(\"\\nScore per fold\")\n",
    "        for i in range(0, len(self.fold_acc)):\n",
    "            print(\n",
    "                \"\\n------------------------------------------------------------------------\"\n",
    "            )\n",
    "            print(\n",
    "                f\"> Fold {i+1} - Loss: {self.fold_loss[i]} - Accuracy: {self.fold_acc[i]}%\"\n",
    "            )\n",
    "        print(\n",
    "            f\"\"\"\n",
    "            \\n\\033[92m\n",
    "            =================================================================\\\\\n",
    "            ********************AVERAGE SCORES FOR ALL FOLDS*****************\\\\\n",
    "            =================================================================\n",
    "            \\033[0m\\n\n",
    "            \"\"\"\n",
    "        )\n",
    "        print(\n",
    "            f\"> Average accuracy: {np.mean(self.fold_acc)} (+- {np.std(self.fold_acc)})\"\n",
    "        )\n",
    "        print(f\"> Average loss: {np.mean(self.fold_loss)}\")\n",
    "        print(\n",
    "            \"\"\"\\n\\033[91m=================================================================\n",
    "            ****************************TRAINING DONE************************\n",
    "            =================================================================\\033[0m\\n\"\"\"\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d21aff49",
   "metadata": {},
   "source": [
    "The method is the same as previous but now we need to also prebuild the pretrained models at each k-fol. But this is a pretty good compromise with the added performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "466a02e6",
   "metadata": {},
   "source": [
    "## Performances analyse\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
